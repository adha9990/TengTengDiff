#!/usr/bin/env python3
"""
è¦–è¦ºåŒ– Diffusion Model çš„ Cross-Attention Maps
ä½¿ç”¨å°ˆæ¡ˆå…§å»ºçš„ AttentionStore åŠŸèƒ½
"""

import argparse
import torch
import os
import matplotlib.pyplot as plt
import numpy as np
from PIL import Image
import cv2
from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler
from utils.ptp_utils import AttentionStore, register_attention_control, aggregate_attention


def parse_args():
    parser = argparse.ArgumentParser(description="è¦–è¦ºåŒ– cross-attention maps")
    parser.add_argument(
        "--model_name",
        type=str,
        default="models/stable-diffusion-v1-5",
        help="åŸºç¤ SD æ¨¡å‹è·¯å¾‘"
    )
    parser.add_argument(
        "--lora_weights",
        type=str,
        required=True,
        help="LoRA æ¬Šé‡è·¯å¾‘ï¼ˆä¾‹å¦‚ï¼šall_generate/hazelnut/full/checkpoint-5000ï¼‰"
    )
    parser.add_argument(
        "--prompt",
        type=str,
        default="a vfx",
        help="ç”Ÿæˆæç¤ºè©"
    )
    parser.add_argument(
        "--num_inference_steps",
        type=int,
        default=50,
        help="æ¨è«–æ­¥æ•¸"
    )
    parser.add_argument(
        "--output_dir",
        type=str,
        default="attention_maps",
        help="è¼¸å‡ºç›®éŒ„"
    )
    parser.add_argument(
        "--attention_res",
        type=int,
        default=16,
        help="Attention map è§£æåº¦ï¼ˆ16 æˆ– 32ï¼‰"
    )
    parser.add_argument(
        "--seed",
        type=int,
        default=42,
        help="éš¨æ©Ÿç¨®å­"
    )
    parser.add_argument(
        "--guidance_scale",
        type=float,
        default=7.5,
        help="Guidance scale"
    )

    return parser.parse_args()


def overlay_attention_on_image(image, attention_map, alpha=0.6, colormap=cv2.COLORMAP_JET):
    """
    å°‡ attention map ç–ŠåŠ åœ¨åœ–ç‰‡ä¸Š

    Args:
        image: PIL Image æˆ– numpy array
        attention_map: 2D attention map (numpy array)
        alpha: ç–ŠåŠ é€æ˜åº¦
        colormap: OpenCV colormap

    Returns:
        ç–ŠåŠ å¾Œçš„ PIL Image
    """
    # è½‰æ›åœ–ç‰‡ç‚º numpy array
    if isinstance(image, Image.Image):
        img_array = np.array(image)
    else:
        img_array = image

    # ç¢ºä¿åœ–ç‰‡æ˜¯ RGB
    if img_array.shape[-1] == 4:  # RGBA
        img_array = img_array[:, :, :3]

    # å°‡ attention map èª¿æ•´åˆ°åœ–ç‰‡å¤§å°
    h, w = img_array.shape[:2]
    attention_resized = cv2.resize(attention_map, (w, h), interpolation=cv2.INTER_CUBIC)

    # æ­£è¦åŒ– attention map åˆ° 0-255
    attention_norm = (attention_resized - attention_resized.min()) / (attention_resized.max() - attention_resized.min() + 1e-8)
    attention_norm = (attention_norm * 255).astype(np.uint8)

    # æ‡‰ç”¨ colormap
    attention_colored = cv2.applyColorMap(attention_norm, colormap)
    attention_colored = cv2.cvtColor(attention_colored, cv2.COLOR_BGR2RGB)

    # ç–ŠåŠ 
    overlay = cv2.addWeighted(img_array, 1 - alpha, attention_colored, alpha, 0)

    return Image.fromarray(overlay)


def create_comparison_view(image, attention_maps, tokens, output_path, top_k=6):
    """
    å‰µå»ºå°æ¯”è¦–åœ–ï¼šåŸåœ– + top-k tokens çš„ attention maps

    Args:
        image: ç”Ÿæˆçš„åœ–ç‰‡
        attention_maps: attention map é™£åˆ— [H, W, num_tokens]
        tokens: token åˆ—è¡¨
        output_path: è¼¸å‡ºè·¯å¾‘
        top_k: é¡¯ç¤ºå‰ k å€‹é‡è¦çš„ tokens
    """
    # è¨ˆç®—æ¯å€‹ token çš„å¹³å‡ attention
    token_importance = attention_maps.mean(axis=(0, 1))

    # æ‰¾å‡º top-k tokensï¼ˆæ’é™¤ start/end tokensï¼‰
    valid_indices = []
    for idx in range(len(tokens)):
        token = tokens[idx]
        if token not in ['<|startoftext|>', '<|endoftext|>'] and not token.startswith('<pad_'):
            valid_indices.append(idx)

    # å¦‚æœæ²’æœ‰æœ‰æ•ˆ tokensï¼Œä½¿ç”¨æ‰€æœ‰é padding tokens
    if not valid_indices:
        valid_indices = [i for i in range(min(len(tokens), attention_maps.shape[2]))]

    # æŒ‰é‡è¦æ€§æ’åº
    valid_importance = [(idx, token_importance[idx]) for idx in valid_indices if idx < len(token_importance)]
    valid_importance.sort(key=lambda x: x[1], reverse=True)

    # é¸æ“‡ top-k
    top_indices = [idx for idx, _ in valid_importance[:top_k]]

    # å¦‚æœä¸è¶³ top_kï¼Œæ·»åŠ æœ€é‡è¦çš„ tokensï¼ˆåŒ…æ‹¬ special tokensï¼‰
    if len(top_indices) < top_k:
        all_indices_sorted = np.argsort(token_importance)[::-1]
        for idx in all_indices_sorted:
            if idx not in top_indices and len(top_indices) < top_k:
                top_indices.append(idx)

    # å‰µå»ºè¦–è¦ºåŒ–
    num_plots = len(top_indices) + 1  # +1 for original image
    num_cols = min(4, num_plots)
    num_rows = (num_plots + num_cols - 1) // num_cols

    fig, axes = plt.subplots(num_rows, num_cols, figsize=(num_cols * 4, num_rows * 4))
    axes = axes.flatten() if num_plots > 1 else [axes]

    # é¡¯ç¤ºåŸåœ–
    axes[0].imshow(image)
    axes[0].set_title("Generated Image", fontsize=12, fontweight='bold')
    axes[0].axis('off')

    # é¡¯ç¤º top-k tokens çš„ attention maps
    for plot_idx, token_idx in enumerate(top_indices, start=1):
        if plot_idx >= len(axes):
            break

        token_label = tokens[token_idx] if token_idx < len(tokens) else f"<pad_{token_idx}>"
        attn_map = attention_maps[:, :, token_idx]
        importance = token_importance[token_idx]

        # ç–ŠåŠ  attention åœ¨åœ–ç‰‡ä¸Š
        overlay = overlay_attention_on_image(image, attn_map, alpha=0.5)

        axes[plot_idx].imshow(overlay)
        axes[plot_idx].set_title(
            f"Token {token_idx}: '{token_label}'\nImportance: {importance:.4f}",
            fontsize=10
        )
        axes[plot_idx].axis('off')

    # éš±è—å¤šé¤˜çš„å­åœ–
    for idx in range(num_plots, len(axes)):
        axes[idx].axis('off')

    plt.tight_layout()
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    plt.close()

    print(f"âœ… å°æ¯”è¦–åœ–å·²å„²å­˜è‡³: {output_path}")


def create_aggregated_heatmap(image, attention_maps, tokens, output_path):
    """
    å‰µå»ºèšåˆæ‰€æœ‰å…§å®¹ tokens çš„ç†±åŠ›åœ–

    Args:
        image: ç”Ÿæˆçš„åœ–ç‰‡
        attention_maps: attention map é™£åˆ— [H, W, num_tokens]
        tokens: token åˆ—è¡¨
        output_path: è¼¸å‡ºè·¯å¾‘
    """
    # æ‰¾å‡ºå…§å®¹ tokensï¼ˆæ’é™¤ special tokens å’Œ paddingï¼‰
    content_indices = []
    for idx in range(min(len(tokens), attention_maps.shape[2])):
        token = tokens[idx] if idx < len(tokens) else f"<pad_{idx}>"
        if token not in ['<|startoftext|>', '<|endoftext|>'] and not token.startswith('<pad_'):
            content_indices.append(idx)

    if not content_indices:
        print("âš ï¸  è­¦å‘Š: æ²’æœ‰æ‰¾åˆ°å…§å®¹ tokensï¼Œä½¿ç”¨æ‰€æœ‰ tokens")
        content_indices = list(range(min(len(tokens), attention_maps.shape[2])))

    # èšåˆå…§å®¹ tokens çš„ attention
    aggregated = attention_maps[:, :, content_indices].sum(axis=2)

    # å‰µå»ºè¦–è¦ºåŒ–
    fig, axes = plt.subplots(1, 3, figsize=(15, 5))

    # åŸåœ–
    axes[0].imshow(image)
    axes[0].set_title("Generated Image", fontsize=12, fontweight='bold')
    axes[0].axis('off')

    # ç´”ç†±åŠ›åœ–
    im = axes[1].imshow(aggregated, cmap='jet', interpolation='bilinear')
    axes[1].set_title("Content Tokens Attention\n(Aggregated)", fontsize=12, fontweight='bold')
    axes[1].axis('off')
    plt.colorbar(im, ax=axes[1], fraction=0.046, pad=0.04)

    # ç–ŠåŠ è¦–åœ–
    overlay = overlay_attention_on_image(image, aggregated, alpha=0.6)
    axes[2].imshow(overlay)
    axes[2].set_title("Overlay on Image", fontsize=12, fontweight='bold')
    axes[2].axis('off')

    # æ·»åŠ  tokens è³‡è¨Š
    content_tokens = [tokens[i] if i < len(tokens) else f"<pad_{i}>" for i in content_indices]
    fig.suptitle(f"Aggregated tokens: {', '.join(content_tokens)}", fontsize=10, y=0.02)

    plt.tight_layout()
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    plt.close()

    print(f"âœ… èšåˆç†±åŠ›åœ–å·²å„²å­˜è‡³: {output_path}")


def visualize_cross_attention(
    attention_maps,
    tokens,
    output_path,
    res=16,
    from_where=["up", "down", "mid"]
):
    """
    è¦–è¦ºåŒ– cross-attention maps

    Args:
        attention_maps: AttentionStore ç‰©ä»¶
        tokens: æç¤ºè©çš„ token åˆ—è¡¨
        output_path: è¼¸å‡ºåœ–ç‰‡è·¯å¾‘
        res: attention map è§£æåº¦
        from_where: è¦èšåˆçš„ UNet ä½ç½®
    """
    # èšåˆ attention maps
    try:
        attention = aggregate_attention(
            attention_store=attention_maps,
            res=res,
            from_where=from_where,
            is_cross=True,
            select=0  # é¸æ“‡ç¬¬ä¸€å€‹æ¨£æœ¬
        )
    except RuntimeError as e:
        if "expected a non-empty list" in str(e):
            print(f"âš ï¸  è­¦å‘Š: åœ¨ {from_where} å±¤æ‰¾ä¸åˆ°è§£æåº¦ {res}x{res} çš„ attention mapsï¼Œè·³éæ­¤è¦–è¦ºåŒ–")
            return
        else:
            raise

    # attention shape: [res, res, num_tokens]
    total_tokens = attention.shape[-1]

    # åªé¡¯ç¤ºå¯¦éš›çš„ prompt tokensï¼ˆä¸åŒ…æ‹¬ paddingï¼‰
    actual_token_count = min(len(tokens), total_tokens)
    display_tokens = actual_token_count  # åªé¡¯ç¤ºå¯¦éš› tokens

    # å‰µå»ºè¦–è¦ºåŒ–ç¶²æ ¼
    num_cols = min(8, display_tokens)
    num_rows = (display_tokens + num_cols - 1) // num_cols

    fig, axes = plt.subplots(num_rows, num_cols, figsize=(num_cols * 3, num_rows * 3))
    if display_tokens == 1:
        axes = np.array([axes])
    elif num_rows == 1:
        axes = axes.reshape(1, -1)

    axes = axes.flatten()

    for idx in range(display_tokens):
        ax = axes[idx]

        # ç²å–ç•¶å‰ token çš„ attention map
        attn_map = attention[:, :, idx].cpu().numpy()

        # é¡¯ç¤º attention map
        im = ax.imshow(attn_map, cmap='jet', interpolation='bilinear')

        # è™•ç† token æ¨™ç±¤
        token_label = tokens[idx] if idx < len(tokens) else f"<pad_{idx}>"

        # è¨ˆç®—æ­¤ token çš„ attention å¼·åº¦
        avg_attention = attn_map.mean()

        ax.set_title(f"Token {idx}: '{token_label}'\nAvg: {avg_attention:.4f}", fontsize=10)
        ax.axis('off')
        plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)

    # éš±è—å¤šé¤˜çš„å­åœ–
    for idx in range(display_tokens, len(axes)):
        axes[idx].axis('off')

    # æ·»åŠ æ•´é«”æ¨™é¡Œ
    location_str = '+'.join(from_where)
    fig.suptitle(f"Cross-Attention Maps ({location_str} layers, {res}x{res})", fontsize=14, y=1.0)

    plt.tight_layout()
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    plt.close()

    print(f"âœ… Attention map å·²å„²å­˜è‡³: {output_path} (é¡¯ç¤º {display_tokens}/{total_tokens} tokens)")


def save_attention_summary(
    attention_maps,
    tokens,
    output_path,
    res=16,
    from_where=["up", "down", "mid"]
):
    """
    å„²å­˜æ¯å€‹ token çš„å¹³å‡ attention å¼·åº¦æ‘˜è¦
    """
    try:
        attention = aggregate_attention(
            attention_store=attention_maps,
            res=res,
            from_where=from_where,
            is_cross=True,
            select=0
        )
    except RuntimeError as e:
        if "expected a non-empty list" in str(e):
            print(f"âš ï¸  è­¦å‘Š: åœ¨ {from_where} å±¤æ‰¾ä¸åˆ°è§£æåº¦ {res}x{res} çš„ attention mapsï¼Œè·³éæ‘˜è¦ç”Ÿæˆ")
            return
        else:
            raise

    # è¨ˆç®—æ¯å€‹ token çš„å¹³å‡ attention
    token_attention = attention.mean(dim=[0, 1]).cpu().numpy()

    # å„²å­˜æ‘˜è¦
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write("Token Attention Summary\n")
        f.write("=" * 50 + "\n\n")

        # è™•ç† token æ•¸é‡å¯èƒ½ä¸åŒ¹é…çš„æƒ…æ³
        num_attn_tokens = len(token_attention)
        for idx in range(num_attn_tokens):
            token_label = tokens[idx] if idx < len(tokens) else f"<pad_{idx}>"
            attn_value = token_attention[idx]
            f.write(f"Token {idx:2d} | {token_label:15s} | Attention: {attn_value:.6f}\n")

        f.write("\n" + "=" * 50 + "\n")
        f.write(f"Total tokens (prompt): {len(tokens)}\n")
        f.write(f"Total tokens (attention): {num_attn_tokens}\n")
        f.write(f"Resolution: {res}x{res}\n")
        f.write(f"Aggregated from: {', '.join(from_where)}\n")

    print(f"âœ… Attention æ‘˜è¦å·²å„²å­˜è‡³: {output_path}")


def main():
    args = parse_args()

    # è¨­å®šéš¨æ©Ÿç¨®å­
    torch.manual_seed(args.seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(args.seed)

    # å‰µå»ºè¼¸å‡ºç›®éŒ„
    os.makedirs(args.output_dir, exist_ok=True)

    print("=" * 60)
    print("Cross-Attention Map è¦–è¦ºåŒ–å·¥å…·")
    print("=" * 60)
    print(f"æ¨¡å‹: {args.model_name}")
    print(f"LoRA: {args.lora_weights}")
    print(f"æç¤ºè©: {args.prompt}")
    print(f"æ¨è«–æ­¥æ•¸: {args.num_inference_steps}")
    print(f"Attention è§£æåº¦: {args.attention_res}x{args.attention_res}")
    print("=" * 60 + "\n")

    # è¼‰å…¥ pipeline
    print("ğŸ“¥ è¼‰å…¥ Stable Diffusion pipeline...")
    pipe = StableDiffusionPipeline.from_pretrained(
        args.model_name,
        torch_dtype=torch.float32,
        safety_checker=None,
        requires_safety_checker=False
    )

    # è¨­å®š scheduler
    scheduler_args = {}
    if "variance_type" in pipe.scheduler.config:
        variance_type = pipe.scheduler.config.variance_type
        if variance_type in ["learned", "learned_range"]:
            variance_type = "fixed_small"
        scheduler_args["variance_type"] = variance_type

    pipe.scheduler = DPMSolverMultistepScheduler.from_config(
        pipe.scheduler.config, **scheduler_args
    )

    # è¼‰å…¥ LoRA æ¬Šé‡
    print(f"ğŸ“¥ è¼‰å…¥ LoRA æ¬Šé‡: {args.lora_weights}")
    pipe.load_lora_weights(args.lora_weights)

    # ç§»è‡³ GPU
    pipe = pipe.to("cuda")

    # å‰µå»º AttentionStore ä¸¦è¨»å†Š
    print("ğŸ”§ è¨»å†Š attention control...")
    attention_store = AttentionStore(save_global_store=True)
    register_attention_control(pipe, attention_store)

    # Tokenize æç¤ºè©ä»¥ç²å– token è³‡è¨Š
    tokens = pipe.tokenizer.encode(args.prompt)
    token_strings = [pipe.tokenizer.decode([token]) for token in tokens]

    print(f"\nğŸ“ æç¤ºè© tokens ({len(token_strings)} å€‹):")
    for idx, token in enumerate(token_strings):
        print(f"  {idx:2d}: {token}")
    print()

    # ç”Ÿæˆåœ–ç‰‡
    print("ğŸ¨ ç”Ÿæˆåœ–ç‰‡ä¸¦æ”¶é›† attention maps...")
    generator = torch.Generator(device="cuda").manual_seed(args.seed)

    result = pipe(
        prompt=args.prompt,
        num_inference_steps=args.num_inference_steps,
        guidance_scale=args.guidance_scale,
        generator=generator
    ).images[0]

    # å„²å­˜ç”Ÿæˆçš„åœ–ç‰‡
    output_image_path = os.path.join(args.output_dir, "generated_image.png")
    result.save(output_image_path)
    print(f"âœ… ç”Ÿæˆåœ–ç‰‡å·²å„²å­˜è‡³: {output_image_path}")

    # è¦–è¦ºåŒ– cross-attention maps
    print("\nğŸ“Š è¦–è¦ºåŒ– cross-attention maps...")

    # 1. å‰µå»ºèšåˆç†±åŠ›åœ–ï¼ˆæœ€é‡è¦çš„è¦–è¦ºåŒ–ï¼‰
    print("\nğŸ”¥ ç”Ÿæˆèšåˆç†±åŠ›åœ–...")
    try:
        attention_all = aggregate_attention(
            attention_store=attention_store,
            res=args.attention_res,
            from_where=["up", "down"],  # ä½¿ç”¨ up + downï¼Œmid å¯èƒ½æ²’æœ‰å°æ‡‰è§£æåº¦
            is_cross=True,
            select=0
        )
        attention_np = attention_all.cpu().numpy()

        aggregated_path = os.path.join(args.output_dir, "attention_aggregated.png")
        create_aggregated_heatmap(result, attention_np, token_strings, aggregated_path)
    except RuntimeError as e:
        print(f"âš ï¸  è­¦å‘Š: ç„¡æ³•ç”Ÿæˆèšåˆç†±åŠ›åœ–: {e}")

    # 2. å‰µå»º top-k tokens å°æ¯”è¦–åœ–
    print("\nğŸ“Š ç”Ÿæˆ Top-K Tokens å°æ¯”è¦–åœ–...")
    try:
        attention_all = aggregate_attention(
            attention_store=attention_store,
            res=args.attention_res,
            from_where=["up", "down"],
            is_cross=True,
            select=0
        )
        attention_np = attention_all.cpu().numpy()

        comparison_path = os.path.join(args.output_dir, "attention_comparison.png")
        create_comparison_view(result, attention_np, token_strings, comparison_path, top_k=6)
    except RuntimeError as e:
        print(f"âš ï¸  è­¦å‘Š: ç„¡æ³•ç”Ÿæˆå°æ¯”è¦–åœ–: {e}")

    # 3. ç‚ºä¸åŒçš„ UNet ä½ç½®å‰µå»ºè©³ç´°è¦–è¦ºåŒ–
    print("\nğŸ—ºï¸  ç”Ÿæˆå„å±¤è©³ç´° attention maps...")
    locations = [
        (["up", "down"], "all"),  # åˆä½µ up å’Œ down
        (["down"], "down"),
        (["up"], "up"),
    ]

    for from_where, name in locations:
        output_path = os.path.join(args.output_dir, f"attention_detail_{name}.png")
        visualize_cross_attention(
            attention_maps=attention_store,
            tokens=token_strings,
            output_path=output_path,
            res=args.attention_res,
            from_where=from_where
        )

    # 4. å„²å­˜ attention æ‘˜è¦
    print("\nğŸ“ ç”Ÿæˆ attention æ‘˜è¦...")
    summary_path = os.path.join(args.output_dir, "attention_summary.txt")
    save_attention_summary(
        attention_maps=attention_store,
        tokens=token_strings,
        output_path=summary_path,
        res=args.attention_res,
        from_where=["up", "down"]
    )

    print("\n" + "=" * 60)
    print("âœ¨ å®Œæˆï¼æ‰€æœ‰æª”æ¡ˆå·²å„²å­˜è‡³:", args.output_dir)
    print("=" * 60)

    # åˆ—å‡ºè¼¸å‡ºæª”æ¡ˆ
    print("\nğŸ“‚ è¼¸å‡ºæª”æ¡ˆ:")
    for filename in sorted(os.listdir(args.output_dir)):
        filepath = os.path.join(args.output_dir, filename)
        size = os.path.getsize(filepath)
        print(f"  â€¢ {filename} ({size / 1024:.1f} KB)")


if __name__ == "__main__":
    main()
